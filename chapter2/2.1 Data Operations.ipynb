{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1Data Operations 数据操作\n",
    "主要学习tensor\n",
    "存储以及操作数据\n",
    "\n",
    "下载的问题\n",
    "\n",
    "pip install numpy==1.21.5 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "pip install pandas==1.2.4 -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1入门"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor其实就是一个代表任意维度的向量，只不过有一些特性而已\n",
    "可以用arange函数创建一个由0到n-1的向量,没有指定的化存储在cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(12)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  3.,  5.,  7.,  9., 11.], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.arange(1,12,2,dtype=float)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape函数可以改变tensor的形状\n",
    "-1代表这个数由torch自动计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.reshape(3,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape可以知道tensor的形状\n",
    "numel可以知道tensor的元素的总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x's shape is torch.Size([3, 4]) \n",
      "x's shape is <built-in method numel of Tensor object at 0x0000017E4A84B9A0> \n"
     ]
    }
   ],
   "source": [
    "print(f\"x's shape is {x.shape} \")\n",
    "print(f\"x's shape is {x.numel} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arange函数得到一个等差序列\n",
    "\n",
    "zeros函数得到全0的tensor\n",
    "\n",
    "ones函数得到全1的\n",
    "\n",
    "randn函数得到正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2051,  1.2356, -0.4490, -2.0856],\n",
       "        [ 0.4181,  1.7530,  0.5186,  0.0306],\n",
       "        [-0.8931,  0.7663, -0.4800,  0.6271]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2运算符 operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+、-、*、/、**\n",
    "\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  # **运算符是求幂运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat函数，很重要用来拼接tensor\n",
    "\n",
    "如果 torch.cat() 处理多个张量，它们在除了拼接维度外的其他维度必须形状一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is \n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "Y is \n",
      "tensor([[2., 1., 4., 3.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [4., 3., 2., 1.]])\n",
      "X 和 Y 按照行拼接\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "X 和 Y 按照列拼接\n",
      "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "X=torch.arange(12,dtype=torch.float32).reshape(3,4)\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(f\"X is \\n{X}\")\n",
    "print(f\"Y is \\n{Y}\")\n",
    "print(f\"X 和 Y 按照行拼接\\n{torch.cat([X,Y],dim=0)}\")\n",
    "print(f\"X 和 Y 按照列拼接\\n{torch.cat([X,Y],dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X==Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 broadcasting mechanism 广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：\n",
    "\n",
    "通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；\n",
    "\n",
    "对生成的数组执行按元素操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 遵循 NumPy 的广播规则\n",
    "\n",
    "1. 如果两个张量的维度数量不同，则自动在较小的形状前面补 1（不会改变原张量）。\n",
    "\n",
    "2. 如果两个张量在某个维度上的大小不同：\n",
    "\n",
    "   - 如果其中一个为 1，则会被扩展为与另一个相同的大小。\n",
    "\n",
    "   - 如果两者都大于 1，但不相等，则报错。\n",
    "\n",
    "3. 当所有维度对齐后，按照较大张量的形状进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1], [2], [3]])  # shape: (3,1)\n",
    "b = torch.tensor([1, 2, 3])  # shape: (3,)\n",
    "\n",
    "print(a + b)  # 广播 b -> (1,3)，然后计算(3,1)+(1,3)->(3,3)+(3,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 4, 6],\n",
      "        [5, 7, 9]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])  # shape: (2,3)\n",
    "B = torch.tensor([1, 2, 3])  # shape: (3,)\n",
    "\n",
    "print(A + B)  # 广播 B -> (1,3) ->(2,3)\n",
    "\n",
    "A = torch.randn(2, 3, 1)  # shape: (2,3,1)\n",
    "B = torch.randn(1, 3, 4)  # shape: (1,3,4)\n",
    "\n",
    "C = A + B  # 广播 A -> (2,3,4)\n",
    "print(C.shape)  # 输出 (2,3,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播失败\n",
    "\n",
    "如果两个张量某个维度大小不匹配，且都不为 1，广播就会失败"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\VScode_workspace\\d2l-zh\\test_gyk\\chapter2\\Data Operations.ipynb 单元格 30\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/VScode_workspace/d2l-zh/test_gyk/chapter2/Data%20Operations.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/VScode_workspace/d2l-zh/test_gyk/chapter2/Data%20Operations.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m4\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/VScode_workspace/d2l-zh/test_gyk/chapter2/Data%20Operations.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(a \u001b[39m+\u001b[39;49m b)  \u001b[39m# ❌ 运行时报错\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "b = torch.randn(4, 3)\n",
    "\n",
    "print(a + b)  # 运行时报错\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播机制中的增加维度其实是用unsqueeze来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a+b= tensor([[1., 1., 1.]])\n",
      "a's shape is torch.Size([1, 3])\n",
      "a+b= tensor([[1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.zeros(3)\n",
    "b=torch.ones(1,3)\n",
    "print(f\"a+b= {a+b}\")\n",
    "a=a.unsqueeze(0)\n",
    "print(f\"a's shape is {a.shape}\")\n",
    "print(f\"a+b= {a+b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要增加新维度（而不是合并），可以使用 torch.stack()。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 4])\n",
      "torch.Size([2, 2, 3, 4])\n",
      "z_cat is tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "z_stack is tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 3, 4)\n",
    "y = torch.zeros(2, 3, 4)\n",
    "\n",
    "z_cat = torch.cat([x, y], dim=0)  # 形状 [4, 3, 4]\n",
    "z_stack = torch.stack([x, y], dim=0)  # 形状 [2, 2, 3, 4]\n",
    "\n",
    "print(z_cat.shape)   # torch.Size([4, 3, 4])\n",
    "print(z_stack.shape) # torch.Size([2, 2, 3, 4])\n",
    "print(f\"z_cat is {z_cat}\")\n",
    "print(f\"z_stack is {z_stack}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.6 numpy & tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.arange(8).reshape(2,4)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码出错的原因是 torch.tensor() 无法自动推断 NumPy 数组的数据类型，需要显式指定数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of numpy.int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\VScode_workspace\\d2l-zh\\test_gyk\\chapter2\\Data Operations.ipynb 单元格 39\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/VScode_workspace/d2l-zh/test_gyk/chapter2/Data%20Operations.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mtensor(a)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/VScode_workspace/d2l-zh/test_gyk/chapter2/Data%20Operations.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m a\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of numpy.int64"
     ]
    }
   ],
   "source": [
    "a=torch.tensor(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3],\n",
       "        [4, 5, 6, 7]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor(a,dtype=torch.int64)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| 方法                  | 适用情况            | 内存共享   | 需要手动指定 `dtype` |\n",
    "| --------------------- | ------------------- | ---------- | -------------------- |\n",
    "| `torch.tensor(a)`     | 适用于所有情况      | ❌ 复制数据 | 需要                 |\n",
    "| `torch.from_numpy(a)` | **NumPy → PyTorch** | ✅ 共享数据 | 不需要               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\VScode_workspace\\d2l-zh\\test_gyk\\chapter2\\Data Operations.ipynb 单元格 42\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/VScode_workspace/d2l-zh/test_gyk/chapter2/Data%20Operations.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mfrom_numpy(a)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/VScode_workspace/d2l-zh/test_gyk/chapter2/Data%20Operations.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m a\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "a=torch.from_numpy(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尼玛，就逆天，估计是版本问题，草"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
