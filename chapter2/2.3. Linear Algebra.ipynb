{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 线性代数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2向量\n",
    "$$\n",
    "\\mathbf{x}=\\begin{bmatrix}x_1 \\\\x_2 \\\\ \\vdots \\\\x_n \\end{bmatrix}\n",
    "$$\n",
    "---\n",
    "请注意，*维度*（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。\n",
    "为了清楚起见，我们在此明确一下：\n",
    "*向量*或*轴*的维度被用来表示*向量*或*轴*的长度，即向量或轴的元素数量。\n",
    "然而，张量的维度用来表示张量具有的轴数。\n",
    "在这个意义上，张量的某个轴的维数就是这个轴的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(20)\n",
    "print(len(x))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3矩阵\n",
    "\n",
    "\n",
    "\\cdots（Center Dots - 水平省略号）\n",
    "\n",
    "\\vdots（Vertical Dots - 竖直省略号）\n",
    "\n",
    "\\ddots（Diagonal Dots - 斜对角省略号）\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \n",
    "\\\\ a_{21} & a_{22} & \\cdots & a_{2n} \n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots \n",
    "\\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \n",
    "\\\\ \\end{bmatrix}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=torch.arange(20).reshape(4,5)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10, 15],\n",
       "        [ 1,  6, 11, 16],\n",
       "        [ 2,  7, 12, 17],\n",
       "        [ 3,  8, 13, 18],\n",
       "        [ 4,  9, 14, 19]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9]],\n",
       "\n",
       "        [[10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reshape(2,2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.5 张量的基本性质"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A :\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "A[0] is:\n",
      "tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "A=torch.arange(20,dtype=torch.float32).reshape(5,4)\n",
    "B=A.clone()\n",
    "print(f\"A :\\n{A}\")\n",
    "print(f\"A[0] is:\\n{A[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0][0]=1\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0][0]=0\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到B只是A的浅拷贝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**矩阵的点乘**\n",
    "\n",
    "具体而言，[**两个矩阵的按元素乘法称为*Hadamard积*（Hadamard product）（数学符号$\\odot$）**]。\n",
    "对于矩阵$\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$，\n",
    "其中第$i$行和第$j$列的元素是$b_{ij}$。\n",
    "矩阵$\\mathbf{A}$和$\\mathbf{B}$的Hadamard积为：\n",
    "$$\n",
    "\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A*B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.6 降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A.sum()是所有元素的和\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n}a_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), tensor(190.))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape,A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沿着某个维度求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([40., 45., 50., 55.]), tensor([ 6., 22., 38., 54., 70.]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(dim=0),A.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=torch.arange(24).reshape(2,3,4)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来一步步解析 `B.sum(axis=[0,2])` 的计算逻辑。  \n",
    "\n",
    "---\n",
    "\n",
    " **1. `B` 的形状**\n",
    "```python\n",
    "B = torch.arange(24).reshape(2, 3, 4)\n",
    "```\n",
    "`B` 的形状是 `(2, 3, 4)`，可以看作 **2 个 `3×4` 的矩阵**：\n",
    "$$\n",
    "    B = \\begin{bmatrix}\n",
    "    \\begin{bmatrix} 0 & 1 & 2 & 3 \\\\ 4 & 5 & 6 & 7 \\\\ 8 & 9 & 10 & 11 \\end{bmatrix},\n",
    "    \\begin{bmatrix} 12 & 13 & 14 & 15 \\\\ 16 & 17 & 18 & 19 \\\\ 20 & 21 & 22 & 23 \\end{bmatrix}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "维度索引：\n",
    "- `B.shape[0] = 2` → **第 0 维**：包含 2 个 `3×4` 矩阵  \n",
    "- `B.shape[1] = 3` → **第 1 维**：每个矩阵有 3 行  \n",
    "- `B.shape[2] = 4` → **第 2 维**：每行有 4 列  \n",
    "\n",
    "---\n",
    "\n",
    " **2. `B.sum(axis=[0,2])` 计算过程**\n",
    "```python\n",
    "B.sum(axis=[0, 2])\n",
    "```\n",
    "表示 **对第 0 维（矩阵）和第 2 维（列）求和**。\n",
    "\n",
    " **(1) 先对 `axis=0` 求和**\n",
    "对 **第 0 维**（即 2 个矩阵）进行逐元素求和：\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "    0+12 & 1+13 & 2+14 & 3+15 \\\\\n",
    "    4+16 & 5+17 & 6+18 & 7+19 \\\\\n",
    "    8+20 & 9+21 & 10+22 & 11+23\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    12 & 14 & 16 & 18 \\\\\n",
    "    20 & 22 & 24 & 26 \\\\\n",
    "    28 & 30 & 32 & 34\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "这样 `B` 变成一个 `3×4` 的矩阵（形状 `(3,4)`）。\n",
    "\n",
    " **(2) 再对 `axis=2` 求和**\n",
    "对 **第 2 维**（列）求和，相当于对每一行的 4 个元素求和：\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "    12+14+16+18 &= 60, \\\\\n",
    "    20+22+24+26 &= 92, \\\\\n",
    "    28+30+32+34 &= 124.\n",
    "    \\end{aligned}\n",
    "$$\n",
    "最终结果：\n",
    "```\n",
    "tensor([ 60,  92, 124 ])\n",
    "```\n",
    "这个结果的形状是 **(3,)**，对应 **原矩阵的 3 行**。\n",
    "\n",
    "---\n",
    "\n",
    " **3. 总结**\n",
    "- `axis=0`：**对 2 个 `3×4` 矩阵相加**，得到一个 `3×4` 矩阵。\n",
    "- `axis=2`：**对每行的 4 个元素求和**，最终得到 `3` 个数（对应 3 行）。\n",
    "\n",
    "所以，最终输出是：\n",
    "```\n",
    "tensor([60, 92, 124])\n",
    "```\n",
    "对应 **每一行所有元素的总和（跨 2 个原始矩阵）**。\n",
    "\n",
    "**你可以尝试不同的 `axis` 组合，看看它们的变化！🚀**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 60,  92, 124])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.sum(axis=[0,2])\n",
    "#B.sum(dim=[0,2])也是一样的\n",
    "#B.sum(dim=0).sum(dim=1)也是一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.5000), tensor(9.5000))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(),A.sum()/A.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.5000), tensor(9.5000))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(dim=[0,1]),A.mean(dim=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6.1. 非降维求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "        [0.2286, 0.2429, 0.2571, 0.2714]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.7 点积 dot product\n",
    "向量点积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.arange(4,dtype=torch.float32)\n",
    "b=torch.ones(4)\n",
    "b=b.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([2.7183, 2.7183, 2.7183, 2.7183]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*是逐个元素相乘\n",
    "\n",
    "dot才是线性代数的点积，等价于逐个元素相乘然后求和\n",
    "\n",
    "$a^Tb=\\sum_{i=1}^{n} a_i*b_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 2.7183, 5.4366, 8.1548]), tensor(16.3097))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b,torch.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.3097)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{A}\\mathbf{x}$矩阵向量\n",
    "torch.mv(A,x)\n",
    "\n",
    "$\\mathbf{A}\\mathbf{B}$矩阵矩阵\n",
    "torch.MM(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " tensor([[1],\n",
       "         [2],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=torch.arange(12).reshape(3,4)\n",
    "x=torch.tensor([[1],[2],[1],[2]])\n",
    "B=torch.arange(12).reshape(4,3)\n",
    "A,x,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.Size([4, 1]), torch.Size([4, 3]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape,x.shape,B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "vector + matrix @ vector expected, got 1, 2, 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\VScode_workspace\\d2l-zh\\test_gyk\\chapter2\\2.3. Linear Algebra.ipynb 单元格 47\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/VScode_workspace/d2l-zh/test_gyk/chapter2/2.3.%20Linear%20Algebra.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAx is:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mmv(A,x)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/VScode_workspace/d2l-zh/test_gyk/chapter2/2.3.%20Linear%20Algebra.ipynb#Y104sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAB is:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mmm(A,B)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: vector + matrix @ vector expected, got 1, 2, 2"
     ]
    }
   ],
   "source": [
    "print(f\"Ax is:\\n{torch.mv(A,x)}\")\n",
    "print(f\"AB is:\\n{torch.mm(A,B)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里把向量当作特殊的矩阵了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ax is:\n",
      "tensor([[10],\n",
      "        [34],\n",
      "        [58]])\n",
      "AB is:\n",
      "tensor([[ 42,  48,  54],\n",
      "        [114, 136, 158],\n",
      "        [186, 224, 262]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ax is:\\n{torch.mm(A,x)}\")\n",
    "print(f\"AB is:\\n{torch.mm(A,B)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=x.reshape(-1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ax is:\n",
      "tensor([10, 34, 58])\n",
      "AB is:\n",
      "tensor([[ 42,  48,  54],\n",
      "        [114, 136, 158],\n",
      "        [186, 224, 262]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ax is:\\n{torch.mv(A,x)}\")\n",
    "print(f\"AB is:\\n{torch.mm(A,B)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.8 范数\n",
    "\n",
    "$L_2$范数和$L_1$范数都是更一般的$L_p$范数的特例：\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n",
    "\n",
    "类似于向量的$L_2$范数，[**矩阵**]$\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$(**的*Frobenius范数*（Frobenius norm）是矩阵元素平方和的平方根：**)\n",
    "\n",
    "(**$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$**)\n",
    "\n",
    "Frobenius范数满足向量范数的所有性质，它就像是矩阵形向量的$L_2$范数。\n",
    "调用以下函数将计算矩阵的Frobenius范数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.arange(6,dtype=torch.float32).reshape(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L-1范数 is:\n",
      "15.0\n",
      "L-2范数 is:\n",
      "7.416198253631592\n",
      "L-inf范数 is:\n",
      "5.0\n",
      "max item of a is:\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"L-1范数 is:\\n{torch.norm(a,p=1)}\")\n",
    "print(f\"L-2范数 is:\\n{torch.norm(a,p=2)}\")\n",
    "print(f\"L-inf范数 is:\\n{torch.norm(a,p=torch.inf)}\")\n",
    "print(f\"max item of a is:\\n{a.max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无穷范数就是max函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
